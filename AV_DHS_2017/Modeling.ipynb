{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling\n",
    "\n",
    "Here we will have some sample codes and links with respect to modeling section.\n",
    "\n",
    "\n",
    "## Modeling Bigger Datasets \n",
    "\n",
    "1. [FTRL Implementation](https://www.kaggle.com/jiweiliu/ftrl-starter-code/code)\n",
    "2. [LibFFM](https://github.com/guestwalk/libffm)\n",
    "3. [Voapal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)\n",
    "4. [Incremental Learning](http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning)\n",
    "\n",
    "## Time Series Forecasting\n",
    "\n",
    "1. [R Tutorial](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)\n",
    "\n",
    "2. [Python Tutorial](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)\n",
    "\n",
    "\n",
    "## Bayesian Optimization\n",
    "\n",
    "Some python libraries are\n",
    "\n",
    "1. [Hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "\n",
    "2. [Spearmint](https://github.com/JasperSnoek/spearmint)\n",
    "\n",
    "3. [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization) \n",
    "\n",
    "Example code can be seen in this [Kaggle Kernel](https://www.kaggle.com/dreeux/hyperparameter-tuning-using-hyperopt)\n",
    "\n",
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def runRF(train_X, train_y, test_X, test_y=None, test_X2=None, depth=20, leaf=10, feat=0.2):\n",
    "    model = ensemble.RandomForestClassifier(\n",
    "            n_estimators = 1000,\n",
    "                    max_depth = depth,\n",
    "                    min_samples_split = 2,\n",
    "                    min_samples_leaf = leaf,\n",
    "                    max_features =  feat,\n",
    "                    n_jobs = 4,\n",
    "                    random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:,1]\n",
    "    test_preds = model.predict_proba(test_X)[:,1]\n",
    "    test_preds2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.log_loss(train_y, train_preds)\n",
    "    test_loss = metrics.log_loss(test_y, test_preds)\n",
    "    print \"Train and Test loss : \", train_loss, test_loss\n",
    "    return test_preds, test_loss, test_preds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XGBoost / Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, rounds=500, dep=8, eta=0.05):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params['eval_metric'] = 'auc'\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = 0.7\n",
    "    params[\"min_child_weight\"] = 1\n",
    "    params[\"colsample_bytree\"] = 0.7\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"seed\"] = seed_val\n",
    "    #params[\"max_delta_step\"] = 2\n",
    "    #params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "    watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "    model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n",
    "    pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_ntree_limit)\n",
    "    \n",
    "    loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "    return pred_test_y, loss, pred_test_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Neural Networks / Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def runNN(train_X, train_y, test_X, test_y=None, test_X2=None, epochs=100, scale=False):\n",
    "    if scale:\n",
    "        sc = preprocessing.StandardScaler()\n",
    "        all_X = pd.concat([train_X, test_X, test_X2], axis=0)\n",
    "        sc.fit(all_X)\n",
    "        train_X = sc.transform(train_X)\n",
    "        test_X = sc.transform(test_X)\n",
    "        test_X2 = sc.transform(test_X2)\n",
    "\n",
    "    random.seed(12345)\n",
    "    np.random.seed(12345)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_shape=(train_X.shape[1],), init='he_uniform')) #, W_regularizer=regularizers.l1(0.002)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    #model.add(Dense(50, init='he_uniform'))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "\n",
    "    #model.add(Dense(100, init='he_uniform'))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(1, init='he_uniform'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adagrad')\n",
    "    \n",
    "    ### Model fitting takes place ###\n",
    "    model.fit(train_X, train_y, batch_size=512, nb_epoch=epochs, validation_data=(test_X, test_y), verbose=2, shuffle=True)\n",
    "    \n",
    "    preds = model.predict(test_X, verbose=0)\n",
    "    preds_test2 = model.predict(test_X2, verbose=0)\n",
    "    loss = metrics.log_loss(test_y, preds)\n",
    "    return preds.ravel(), loss, preds_test2.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ensembling\n",
    "\n",
    "Codes for basic ensembling methods can be seen in this [github link by MLWave](https://github.com/MLWave/Kaggle-Ensemble-Guide)\n",
    "\n",
    "## Stacking \n",
    "\n",
    "1. [StackNet](https://github.com/kaz-Anova/StackNet) by Marios KazAnova\n",
    "2. [Stacked Ensembles](https://h2o-release.s3.amazonaws.com/h2o/rel-ueno/2/docs-website/h2o-docs/data-science/stacked-ensembles.html) by H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
